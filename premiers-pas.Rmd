---
title: "Premiers pas"
author: "Timothée Flutre (INRA)"
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
lang: "fr"
colorlinks: true
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: TRUE
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
mode: selfcontained
abstract: |
  Ce document a pour but d'introduire concrètement les étudiants à l'un des aspects de la modélisation statistique, la simulation. Il prend comme exemple la régression linéaire simple, historiquement développée par [Galton (1886)](http://dx.doi.org/10.2307/2841583) pour étudier l'hérédité de la taille dans l'espèce humaine.
---

<!--
Ce morceau de code R est utilisé pour vérifier que tout ce dont on a besoin est disponible.
-->
```{r setup, include=FALSE}
##`
## https://github.com/vspinu/polymode/issues/147#issuecomment-399745611
R.v.maj <- as.numeric(R.version$major)
R.v.min.1 <- as.numeric(strsplit(R.version$minor, "\\.")[[1]][1])
if(R.v.maj < 2 || (R.v.maj == 2 && R.v.min.1 < 15))
  stop("requires R >= 2.15", call.=FALSE)

suppressPackageStartupMessages(library(knitr))
opts_chunk$set(echo=TRUE, warning=TRUE, message=TRUE, cache=FALSE)

options(digits=3)
```


# Préambule

Ce document a été généré à partir d'un fichier texte au format Rmd utilisé avec le logiciel libre [R](http://www.r-project.org/).
Pour exporter un tel fichier vers les formats HTML et PDF, installez le paquet [rmarkdown](http://cran.r-project.org/web/packages/rmarkdown/index.html) (il va vraisemblablement vous être demandé d'installer d'autres paquets), puis ouvrez R et entrez:
```{r ex_rmd, eval=FALSE}
library(rmarkdown)
render("premiers-pas.Rmd", "all")
```

Il est généralement plus simple d'utiliser le logiciel libre [RStudio](http://www.rstudio.com/), mais ce n'est pas obligatoire.
Pour plus de détails, lisez [cette page](http://rmarkdown.rstudio.com/).

Le format Rmd permet également d'utiliser le language [LaTeX](https://www.latex-project.org/) pour écrire des équations.
Pour en savoir plus, reportez-vous au [livre en ligne](https://fr.wikibooks.org/wiki/LaTeX).

De plus, ce document nécessite de charger des paquets additionnels (ceux-ci doivent être installés au préalable sur votre machine, via \verb+install.packages("pkg")+):

```{r load_pkg}
suppressPackageStartupMessages(library(MASS))
```

Il est également utile de savoir combien de temps est nécessaire pour exécuter tout le code R de ce document (voir l'annexe):
```{r time_0}
t0 <- proc.time()
```


# Contexte

Ce document fait partie de l'atelier "Prédiction Génomique" organisé et animé par Jacques David et Timothée Flutre depuis 2015, avec l'aide de Julie Fiévet et Philippe Brabant, à [Montpellier SupAgro](http://www.supagro.fr) dans le cadre de l'option [APIMET](http://www.agro-montpellier.fr/web/pages/?idl=19&page=216&id_page=630) (Amélioration des Plantes et Ingénierie végétale Méditerranéennes et Tropicales) couplée à la spécialité SEPMET (Semences Et Plants Méditerranéens Et Tropicaux) du [Master 3A](http://www.supagro.fr/web/pages/?idl=19&page=1689) (Agronomie et Agroalimentaire), et de la spécialisation [PIST](http://www.agroparistech.fr/Production-et-innovation-dans-les,1633.html) du [Cursus Ingénieur](http://www.agroparistech.fr/Cursus-ingenieurs.html) d'[AgroparisTech](http://www.agroparistech.fr/).

Le copyright appartient à Montpellier SupAgro et à l'Institut National de la Recherche Agronomique.
Le contenu du répertoire est sous license [Creative Commons Attribution-ShareAlike 4.0 International](http://creativecommons.org/licenses/by-sa/4.0/).
Veuillez en prendre connaissance et vous y conformer (contactez les auteurs en cas de doute).

Les versions du contenu sont gérées avec le logiciel [git](https://www.git-scm.com/), et le dépôt central est hébergé sur [GitHub](https://github.com/timflutre/atelier-prediction-genomique).


# Introduction

## A propos de modélisation statistique

"Essentially, all models are wrong, but some are useful." (Box, 1987).
Cette célèbre citation illustre parfaitement le fait qu'un modèle est une simplification du phénomène étudié, mais qu'après tout, si cette simplification nous apporte des enseignements et nous permet de prendre de bonnes décisions, cela importe tout autant.

Il est donc important de rappeler que la première question à se poser, en tant que modélisateur, concerne la validité du modèle.
Bien que cela paraisse évident, ceci consiste avant tout à vérifier que les données à analyser correspondent bien à la question à laquelle on veut répondre ([Gelman & Hill, 2006](http://www.worldcat.org/isbn/0521867061)).

Il est également utile, pour mieux comprendre le processus de modélisation statistique, de distinguer le "monde réel", dans lequel vivent les données, du "monde théorique", dans lequel vivent les modèles: "When we use a statistical model to make a statistical inference, we implicitly assert that the variation exhibited by data is captured reasonably well by the statistical model, so that the theoretical world corresponds reasonably well to the real world." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).

En particulier, il ne faut pas confondre les données avec des variables aléatoires, même si on fait souvent le raccourci: "In both approaches [frequentist and Bayesian], a statistical model is introduced and we may say that the inference is based on what *would* happen if the data *were* to be random variables distributed according to the statistical model. This modeling assumption would be reasonable if the model *were* to describe accurately the variation in the data." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).


## Notation et vocabulaire

L'inférence avec un modèle statistique consiste généralement à **estimer les paramètres**, puis à s'en servir pour **prédire de nouvelles données**.

Lorsque l'on propose un modèle pour répondre à une question donnée, on commence donc par expliquer les notations.
Suivant les conventions, nous utilisons des lettres grecques pour dénoter les paramètres (non-observés), par exemple $\theta$, des lettres romaines pour les données observées, par exemple $y$, et des lettres romaines surmontées d'un tilde pour les données prédites, $\tilde{y}$.
L'ensemble des données est généralement noté en majuscule, par exemple $\mathcal{D} = \{ y_1, y_2, y_3 \}$.
De même, l'ensemble des paramètres est généralement noté en majuscule, par exemple $\Theta = \{ \theta_1, \theta_2 \}$.
De plus, s'il y a plusieurs paramètres ou données, ils se retrouvent mathématiquement dans des vecteurs, en gras, ce qui donne $\boldsymbol{\theta}$ et $\boldsymbol{y}$.
Par convention, les vecteurs sont en colonne.

Une fois les notations établies, on écrit la **vraisemblance** (*likelihood*), souvent présentée comme étant la "probabilité des données sachant les paramètres".
En fait, si les données sont des variables continues, c'est la densité de probabilité des données sachant les paramètres, notée $p(y | \theta)$, et si les données sont des variables discrètes, c'est la fonction de masse, notée $P(y | \theta)$.
Mais le plus important est de réaliser que, dans la vraisemblance, ce ne sont pas les données qui varient, mais les paramètres: la vraisemblance est une fonction des paramètres, d'où le fait qu'on la note $\mathcal{L}(\theta)$ ou $\mathcal{L}(\theta | y)$.

Tout naturellement, la méthode du **maximum de vraisemblance** cherche à identifier la valeur du paramètre, notée $\hat{\theta}$ par convention, qui maximise la vraisemblance:

\begin{align}
\hat{\theta} = \text{argmax}_\theta \, \mathcal{L} \; \; \Leftrightarrow \; \; \frac{\partial \mathcal{L}}{\partial \theta}(\hat{\theta}) = 0
\end{align}

Mais ceci n'est que le tout début de la démarche!
En effet, certains décrivent les statistiques comme étant la "science de l'incertitude" ([Lindley, 2000](http://dx.doi.org/10.1111/1467-9884.00238)).
Or pour l'instant, nous n'avons parlé que de la façon d'obtenir *une* valeur par paramètre, celle qui maximise la vraisemblance.
Il est donc primordial ensuite de **quantifier l'incertitude** que nous avons quant à cette valeur.
C'est sur ce point que différentes approches sont possibles (**fréquentiste**, **bayésienne**).
Mais quelle que soit l'approche, si les échantillons sont "représentatifs" et le modèle n'est pas trop mauvais, on peut généralement s'attendre au fait que plus le nombre d'échantillons augmente, plus l'incertitude sur la valeur du paramètre diminue.


## Comprendre la vraisemblance

Le vocabulaire esquissé au paragraphe précédent n'est pas forcément très intuitif.
Par exemple, supposons que l'on étudie une quantité physique dont la valeur résulte de la somme d'une très grande quantité de facteurs indépendants, chacun ayant un faible impact sur la valeur finale.
Prenons trois mesures de cette quantité d'intérêt.
Comme il y a de la variation, on choisit d'introduire une **variable aléatoire** $Y$ correspondant à la quantité d'intérêt, et dénotons par $y_1$, $y_2$ et $y_3$ les trois observations, vues comme des réalisations de cette variable aléatoire.

```{r echo=FALSE}
set.seed(1)
y <- rnorm(n=3, mean=5, sd=1)
```
Par exemple, supposons les valeurs suivantes:

- $y_1$ = `r y[1]`

- $y_2$ = `r y[2]`

- $y_3$ = `r y[3]`

Etant donné les caractéristiques du phénomène ("somme d'une très grande quantité de facteurs indépendants, chacun ayant un faible impact"), il est raisonnable de supposer que la variable $Y$ suit une **loi Normale** (c.f. le théorème central limite).
Cette distribution de probabilité est caractérisée par deux paramètres, sa **moyenne** que l'on note généralement $\mu$, et sa **variance** que l'on note généralement $\sigma^2$ ($\sigma$ étant l'écart-type).
En terme de notation, on écrit $Y \sim \mathcal{N}(\mu, \sigma^2)$, et la densité de probabilité de la réalisation $y$ de $Y$ s'écrit:
\begin{align}
Y \sim \mathcal{N}(\mu, \sigma^2) \; \; \Leftrightarrow \; \; p(Y=y \, | \, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y - \mu)^2}{2 \sigma^2} \right)
\label{eqn:Norm}
\end{align}

L'intérêt de ce modèle paramétrique est de pouvoir "résumer" les données, par exemple un million de mesures, par seulement deux valeurs, les paramètres.
Mais bien entendu, nous ne connaissons pas les valeurs de paramètres!
La moyenne $\mu$ peut prendre toutes les valeurs entre $-\infty$ et $+\infty$, et la variance $\sigma^2$ n'a pour seule restriction que d'être positive.
Or comme le montrent les graphiques ci-dessous, la loi Normale peut être assez différente selon les valeurs de ces paramètres:

```{r effect_mu, echo=FALSE}
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
```

```{r effect_sigma, echo=FALSE}
x <- seq(from=-7, to=13, length.out=500)
plot(x=x, y=dnorm(x=x, mean=3, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(mu==3))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=3, sd=3), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(sigma==1), expression(sigma==3)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
```

Revenons à nos trois mesures: `r y`.
Parmi toutes les valeurs possibles des paramètres, le défi consiste donc à trouver celles pour lesquelles la loi Normale est une bonne description du mécanisme qui a généré ces données.
On parle donc de trouver les valeurs des paramètres de telle sorte que la vraisemblance d'obtenir ces données soit la plus élevée possible pour ces valeurs des paramètres.

Pour rendre les choses plus facile, supposons que l'on connaisse déjà la variance: $\sigma^2 = 1$.
Il ne nous reste plus qu'à trouver la moyenne, $\mu$.
Regardons cela de plus près pour la première observation, $y_1$ = `r y[1]`:

```{r ex_lik_y1, echo=FALSE}
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~vraisemblances~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=2, sd=1))
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=5, sd=1))
segments(x0=y[1], y0=dnorm(x=y[1], mean=2, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=2, sd=1), col="blue")
segments(x0=y[1], y0=dnorm(x=y[1], mean=5, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=5, sd=1), col="red")
text(x=1.1*y[1], y=0.18, labels=expression(y[1]))
text(x=-1.6, y=dnorm(x=y[1], mean=2, sd=1), col="blue", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==2,")")))
text(x=-1.6, y=dnorm(x=y[1], mean=5, sd=1), col="red", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==5,")")))
```

D'après le graphique:
\[
p(y_1 \, | \, \mu=5, \sigma=1) \, > \, p(y_1 \, | \, \mu=2, \sigma=1)
\]

Cela se vérifie si l'on fait le calcul avec la formule \eqref{eqn:Norm}:

* $p(y_1 \, | \, \mu=5, \sigma=1)$ = `r dnorm(x=y[1], mean=5, sd=1)`;

* $p(y_1 \, | \, \mu=2, \sigma=1)$ = `r dnorm(x=y[1], mean=2, sd=1)`.

Comme les deux densités ont la même valeur pour $\sigma$, la différence vient bien du terme $(y - \mu)^2$ dans l'exponentielle de \eqref{eqn:Norm}, terme qui représente l'écart à la moyenne.
Au final, nous pouvons donc dire qu'en ce qui concerne la première observation, la vraisemblance $\mathcal{L}(\mu=5,\sigma=1)$ est plus grande que $\mathcal{L}(\mu=2,\sigma=1)$.

Comme on dispose de plusieurs observations, $\{y_1,y_2,y_3\}$, et qu'on suppose qu'elles sont toutes des réalisations de la même variable aléatoire, $Y$, il est pertinent de calculer la vraisemblance de toutes ces observations conjointement plutôt que séparément:
\begin{align}
\mathcal{L}(\mu, \sigma) = p(y_1,y_2,y_3 \, | \, \mu, \sigma)
\end{align}

Si l'on fait aussi l'hypothèse que ces observations sont indépendantes, cela se simplifie en:
\begin{align}
\mathcal{L}(\mu, \sigma) &= p(y_1 \, | \, \mu, \sigma) \times p(y_2 \, | \, \mu, \sigma) \times p(y_3 \, | \, \mu, \sigma) \\
&= \prod_{i=1}^3 p(y_i \, | \, \mu, \sigma)
\end{align}

Il n'est pas très pratique de maximiser la vraisemblance directement, on préfère passer au log (qui est monotone, donc le maximum de l'un est aussi le maximum de l'autre):
\begin{align}
l(\mu,\sigma) &= \log \mathcal{L}(\mu, \sigma) \\
&= \sum_{i=1}^3 \log p(y_i \, | \, \mu, \sigma) \\
&= \sum_{i=1}^3 \log \left[ \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu)^2}{2 \sigma^2} \right) \right] \\
&= - 3 \log \sigma \; - \frac{3}{2} \log (2\pi) \; - \frac{1}{2 \sigma^2} \sum_{i=1}^3 (y_i - \mu)^2
\end{align}

Lorsque le nombre d'observations augmente, un simple examen graphique n'est pas très pratique ni suffisant.
D'où le fait, qu'en pratique, (i) on écrit une fonction qui calcule la log-vraisemblance, et (ii) on cherche le maximum de cette fonction:

```{r compute_lik}
compute.log.likelihood <- function(parameters, data){
  mu <- parameters[1]
  sigma <- parameters[2]
  y <- data
  n <- length(y)
  log.lik <- - n * log(sigma) - (n/2) * log(2 * pi) - sum(((y - mu)^2) / (2 * sigma^2))
  return(log.lik)
}

compute.log.likelihood(c(5,1), y)
compute.log.likelihood(c(2,1), y)
```

Dans le cas de la loi Normale, il existe déjà dans R des fonctions implémentant la densité de probabilité, ce qui nous permet de vérifier que nous n'avons pas fait d'erreur:
```{r check_compute_lik}
sum(dnorm(x=y, mean=5, sd=1, log=TRUE))
sum(dnorm(x=y, mean=2, sd=1, log=TRUE))
```

Cette section devrait vous avoir donné les bases du raisonnement ainsi que des techniques que nous allons utiliser dans la suite de l'atelier, commençant dès maintenant avec la régression linéaire simple.


# Ecrire le modèle

## Notations

* $n$: nombre d'individus (diploïdes, supposés non-apparentés)

* $i$: indice indiquant le $i$-ème individu, donc $i \in \{1,\ldots,n\}$

* $y_i$: phénotype de l'individu $i$ pour la caractère d'intérêt

* $\mu$: moyenne globale du phénotype des $n$ individus

* $f$: fréquence de l'allèle minoritaire au marqueur SNP d'intérêt (situé sur un autosome)

* $x_i$: génotype de l'individu $i$ à ce SNP, codé comme le nombre de copie(s) de l'allèle minoritaire à ce SNP chez cet individu ($\forall i \; x_i \in \{0,1,2\}$)

* $\beta$: effet additif de chaque copie de l'allèle minoritaire en unité du phénotype

* $\epsilon_i$: erreur pour l'individu $i$

* $\sigma^2$: variance des erreurs

Données: $\mathcal{D} = \{ (y_1 \, | \, x_1), \ldots, (y_n \, | \, x_n) \}$

Paramètres: $\Theta = \{ \mu, \beta, \sigma \}$


## Vraisemblance

Dans notre cas, nous supposons que le génotype au SNP d'intérêt a un effet additif sur la moyenne du phénotype, ce qui s'écrit généralement:
\begin{align}
\forall i \; \; y_i = \mu + \beta x_i + \epsilon_i \text{ avec } \epsilon_i \overset{\text{i.i.d}}{\sim} \mathcal{N}(0, \sigma^2)
\label{eqn:lik}
\end{align}

Une autre façon, mais équivalente, de l'écrire est:
\begin{align}
\forall i \; \; y_i \, | \, x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\label{eqn:likCond}
\end{align}

Cette deuxième formulation montre bien que, de manière générale, si l'on connaît les valeurs des variables explicatives ainsi que celles des paramètres, alors on est capable de simuler des valeurs de réponse.

De plus, on peut écrire la vraisemblance sous forme plus explicite comme une fonction des paramètres $\Theta = \{ \mu, \beta, \sigma \}$:

\begin{align}
\mathcal{L}(\Theta \, | \, \mathcal{D} ) &= p(\boldsymbol{y} \, | \, \boldsymbol{x}, \Theta) \\
&= \prod_{i=1}^n p(y_i \, | \, x_i, \mu, \beta, \sigma) \\
&= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - (\mu + \beta x_i))^2}{2 \sigma^2} \right)
\label{eqn:likDetails}
\end{align}


# Simuler des données

Afin de simuler des données, nous allons utiliser un générateur de nombres pseudo-aléatoires.
Le mot "pseudo" est là pour rappeler que les générateurs informatiques sont déterministes et peuvent donc être initialisés avec une graine (*seed*), très utile pour la reproductibilité des analyses:

```{r set_seed}
set.seed(1866) # année de parution de l'article de Mendel fondant la génétique
```

Commençons par fixer le nombre d'individus, $n$:
```{r set_sample_size}
n <- 200
```

Puis la moyenne globale, $\mu$ (de manière arbitraire, ce n'est pas très important car on peut toujours centrer les phénotypes en début d'analyse):
```{r set_global_mean}
mu <- 50
```

Pour simuler les génotypes, $\boldsymbol{x}$, nous allons supposer que la population est à l'équilibre d'Hardy-Weinberg:
```{r simul_geno}
##' Genotype frequencies
##'
##' Calculate the genotype frequencies at a locus assuming the Hardy-Weinberg equilibrium
##' (https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle).
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
##' @author Timothee Flutre
calcGenoFreq <- function(maf){
  stopifnot(is.numeric(maf), length(maf) == 1, maf >= 0, maf <= 0.5)
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}

f <- 0.3
genotypes <- sample(x=c(0,1,2), size=n, replace=TRUE, prob=calcGenoFreq(f))
```

Le morceau de code ci-dessus vous montre aussi les bonnes pratiques de programmation:

* choisir des noms de fonctions et variables clairs et explicites;

* documenter le code;

* citer des référence si nécessaire;

* vérifier la validité des arguments au début d'une fonction.

Regardons à quoi ressemblent les génotypes que nous venons de simuler:
```{r look_geno}
table(genotypes)
sum(genotypes) / (2 * n) # estimate of the MAF
var(genotypes) # important for the estimate of beta
```

Tirons une valeur pour l'effet du génotype sur le phénotype, $\beta$:
```{r simul_geno_effect}
(beta <- rnorm(n=1, mean=2, sd=1))
```

Simulons les erreurs, $\boldsymbol{\epsilon}$ (par simplicité, fixons $\sigma$ à 1):
```{r simul_errors}
sigma <- 1
errors <- rnorm(n=n, mean=0, sd=sigma)
```

Nous avons maintenant tout ce qu'il faut pour simuler les phénotypes, $\boldsymbol{y}$, via \eqref{eqn:lik}:
```{r simul_pheno}
phenotypes <- mu + beta * genotypes + errors
```

Pour la suite, il est habituel dans R d'organiser les données dans un tableau:
```{r org_data}
dat <- data.frame(x=genotypes, y=phenotypes)
summary(dat)
```


# Réaliser l'inférence

## Visualisation graphique

Avant toute autre chose, regardons à quoi ressemblent les données:
```{r look_pheno}
hist(phenotypes, breaks="FD", las=1, main="",
     xlab=expression(paste("Phénotypes, ", bold(y))),
     ylab="Nombre d'individus")
```

Comme ce qui nous intéresse ici, ce ne sont pas uniquement les phénotypes, mais bien la relation entre les génotypes et les phénotypes, un autre type de graphique semble plus approprié:
```{r look_geno_pheno}
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
```

Remarquez l'importance de faire des graphiques les plus clairs et intelligibles possible.
Ce ne sont souvent que les graphiques que l'on montre à la place des tableaux de résultats, ceux-ci étant connus pour être plus difficiles à lire ([Gelman *et al.*, 2002](http://dx.doi.org/10.1198/000313002317572790)).


## Dérivation mathématique

Pour trouver les valeurs des paramètres qui maximisent la vraisemblance \eqref{eqn:likDetails}, on travaille généralement avec la log-vraisemblance, $l(\Theta) = \log \mathcal{L}(\Theta)$, puis on utilise les [règles d'analyse](https://en.wikipedia.org/wiki/Differentiation_rules):

* $\frac{\partial l}{\partial \beta}(\hat{\beta}) = 0 \; \Leftrightarrow \; \hat{\beta} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2}$

* $\frac{\partial l}{\partial \mu}(\hat{\mu}) = 0 \; \Leftrightarrow \; \hat{\mu} = \bar{y} - \hat{\beta} \bar{x}$

* $\frac{\partial l}{\partial \sigma}(\hat{\sigma}) = 0 \; \Leftrightarrow \; \hat{\sigma} = \frac{1}{n} \sum_i (y_i - (\hat{\mu} + \hat{\beta} x_i))^2$

où $\bar{x} = \frac{1}{n} \sum_i x_i$ et $\bar{y} = \frac{1}{n} \sum_i y_i$.

Une fois ces estimations obtenues, on veut généralement faire deux choses.
La première est d'évaluer la qualité du modèle, c'est-à-dire son ajustement aux données.
Dans le cas de la régression linéaire, ceci peut se faire à l'aide du coefficient de détermination, interprété comme la proportion de la variance phénotypique expliquée par les prédicteurs:

\begin{align}
R^2 = \frac{Var(\hat{\mu} + \hat{\beta} \boldsymbol{x})}{Var(\boldsymbol{y})} = \frac{\hat{\beta}^2 \; Var(\boldsymbol{x})}{\hat{\beta}^2 \; Var(\boldsymbol{x}) + \hat{\sigma}^2} \; \; \Leftrightarrow \; \; R^2 = \frac{\sum_i ((\hat{\mu} + \hat{\beta} x_i) - \bar{y})^2}{\sum_i (y_i - \bar{y})^2}
\label{eqn:R2}
\end{align}

La deuxième est de quantifier l'incertitude autour de ces estimations.
Dans la démarche fréquentiste, on considère que les estimations sont des réalisations de variables aléatoires appelées "estimateurs".
Dans le cas de la régression linéaire, ces estimateurs, $M$ pour $\mu$, $B$ pour $\beta$ et $S$ pour $\sigma$, sont des combinaisons linéaires des $\{Y_i\}$, variables indépendantes et suivant une loi Normale, donc les estimateurs suivent eux aussi une loi Normale.
Par exemple, pour l'estimateur $B$ du paramètre $\beta$:

\begin{align}
B = \frac{\sum_i (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_i (x_i - \bar{x})^2} \; \; \text{ et } \; \; B \sim \mathcal{N}(E_B, V_B)
\end{align}

Il reste alors à calculer l'espérance et la variance de la loi Normale pour chaque estimateur, et c'est notamment à partir de la variance de l'estimateur que l'on obtient l'erreur standard de l'estimation, celle-ci permettant de quantifier l'incertitude autour de l'estimation.

Pour en savoir plus, reportez-vous par exemple au livre "Statistique inférentielle" de Daudin, Robin et Vuillet (Presses Universitaires de Rennes, 2001).
Du matériel pédagogique sur le modèle linéaire est aussi disponible en accès libre sur le site internet de la Société Française de Statistique ([ici](https://www.sfds.asso.fr/fr/enseignement_de_la_statistique/ressources/enseignement_superieur/597-par_thematiques/)).


## Implémentation (facile)

Bien entendu, R a nativement une fonction implémentant l'estimation par maximum de vraisemblance d'un modèle linéaire, [lm](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html):
```{r fit_lm}
fit <- lm(y ~ x, data=dat)
```

Avant toute autre chose, il nous faut vérifier que les hypothèses du modèles sont vérifiées (homoscédasticité, Normalité, indépendance):
```{r diagnostics}
par(mfrow=c(2,2))
plot(fit)
```

Ceci semble être bien le cas (évidemment puisque nous avons simulé nous-même les données !).

Regardons donc les résultats:
```{r get_results_summary}
summary(fit)
```

Rajoutons la droite de régression au graphique des données:
```{r look_geno_pheno_reg}
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright",
       legend=expression(hat(y)[i]==hat(mu)~+~hat(beta)~x[i]),
       col="red", lty=1, bty="n")
```


## Implémentation (plus dure)

Il peut être intéressant, surtout dans ce cas simple, d'implémenter cette méthode par soi-même pour mieux la comprendre.
Pour cela, on peut utiliser la fonction [mle](http://stat.ethz.ch/R-manual/R-devel/library/stats4/html/mle.html) du paquet stats4.

Il faut d'abord écrire une fonction calculant l'opposé de la log-vraisemblance:
```{r neg_log_lik}
negLogLik <- function(mu, beta, sigma){
  - sum(dnorm(x=dat$y, mean=mu + beta * dat$x, sd=sigma, log=TRUE))
}
```

Puis demander à la fonction \verb+mle+ de la maximiser (en spécifiant que le paramètre $\sigma$ ne peut pas être négatif ou nul):
```{r fit_mle}
library(stats4)
fit2 <- mle(negLogLik, start=list(mu=mean(dat$y), beta=0, sigma=1),
            method="L-BFGS-B", nobs=nrow(dat),
            lower=c(-Inf,-Inf,10^(-6)), upper=c(+Inf,+Inf,+Inf))
summary(fit2)
```


# Evaluer les résultats

## Sélection de modèles

Cette première étape a à voir avec l'ajustement du modèle aux données.
Dans notre cas de régression linéaire simple, on peut utiliser le coefficient de détermination:
```{r adjust}
summary(fit)$r.squared
```

On peut facilement vérifier que cette valeur renvoyée par la fonction \verb+lm+ correspond à la formule \eqref{eqn:R2}:
```{r calc_adjust}
(coefficients(fit)[2]^2 * var(dat$x)) /
  (coefficients(fit)[2]^2 * var(dat$x) + summary(fit)$sigma^2)
```

Dans le cas où plusieurs modèles sont intéressants, on peut essayer de vouloir en sélectionner un s'il paraît "meilleur" que les autres.
Cette étape ne sera pas discutée plus avant ici, mais pour en savoir plus, lisez ce que vous trouverez sur internet en cherchant les termes "AIC", "BIC" et "validation croisée" (*cross validation*).


## Estimation de paramètres

Prenons l'exemple du paramètre $\beta$, de son estimateur du maximum de vraisemblance $B$ et de son estimation $\hat{\beta}$.
Comme nous avons simulé les données, nous connaissons la vraie valeur du paramètre:
```{r true_beta}
beta
```

Après avoir ajusté le modèle avec la fonction \verb+lm()+, nous pouvons récupérer l'estimation de ce paramètre:
```{r estim_beta}
(beta.hat <- coefficients(fit)[2])
```

Pour comparer les deux, on définit une fonction de perte (*loss function*) reliant le paramètre, $\beta$, et son estimateur, $B$.
On utilise communément une fonction quadratique, dont on prend l'espérance, ce qui donne l'erreur quadratique moyenne (*mean squared error*):
\begin{align}
MSE = E \left( (B - \beta)^2 \right)
\end{align}

Calculons sa racine carrée afin que le résultat soit dans la même unité que le paramètre:
```{r rmse_beta}
(rmse.beta <- sqrt((beta.hat - beta)^2))
```


## Prédiction de données

Nous pouvons aussi calculer l'erreur quadratique moyenne avec les phénotypes déjà observés (on parle de *in-sample predictions*):
```{r calc_rmse_y}
y <- phenotypes
y.hat <- (coefficients(fit)[1] + coefficients(fit)[2] * genotypes)
errors <- y - y.hat
(rmse.y <- sqrt(mean(errors^2)))
```

Plus facilement, on peut aussi utiliser la fonction \verb+predict+:
```{r rmse_y}
errors <- phenotypes - predict(fit)
(rmse.y <- sqrt(mean(errors^2)))
```

Mais, de façon plus intéressante, nous voudrions évaluer les prédictions phénotypiques sur $n_{\text{new}}$ nouveaux individus.
Pour cela, commençons par simuler de nouvelles données, $\mathcal{D}_{\text{new}} = \{(y_{i,\text{new}} \, | \, x_{i,\text{new}})\}$, toujours avec les *mêmes* "vraies" valeurs des paramètres, $\Theta = \{\mu, \beta, \sigma\}$:
```{r simul_new_data}
set.seed(1944) # année de découverte de l'ADN comme support des gènes
n.new <- 100
x.new <- sample(x=c(0,1,2), size=n.new, replace=TRUE, prob=calcGenoFreq(f))
y.new <- mu + beta * x.new + rnorm(n=n.new, mean=0, sd=sigma)
```

Puis utilisons les estimations des paramètres obtenues précédemment pour prédire les nouveaux phénotypes à partir des nouveaux génotypes, $\tilde{\mathcal{D}}_{\text{new}} = \{(\tilde{y}_{i,\text{new}} = \hat{\mu} + \hat{\beta} \, x_{i,\text{new}})\}$ (*out-of-sample predictions*):
```{r predict_ytilde}
y.new.tilde <- (coefficients(fit)[1] + coefficients(fit)[2] * x.new)
```

Enfin, calculons l'erreur quadratique de prédiction:
```{r rmspe}
errors.tilde <- y.new - y.new.tilde
(rmspe <- sqrt(mean(errors.tilde^2)))
```

Regardons visuellement ce que ça donne:

```{r ynew_vs_ytilde}
plot(x=y.new, y=y.new.tilde, las=1,
     xlab=expression(paste("Nouveaux vrais phénotypes, ", bold(y)[new])),
     ylab=expression(paste("Nouveaux phénotypes prédits, ", bold(tilde(y))[new])))
abline(a=0, b=1, lty=2)
legend("topleft", legend="ligne identité", lty=2, bty="n")
```


# Explorer les simulations possibles

La simulation est un outil particulièrement utile pour explorer comment un modèle répond à des changements dans les données et les paramètres.

On pourrait par exemple avoir envie de savoir ce qui se passe si la taille de l'échantillon, $n$, varie.
Idem, que se passe-t-il si, à $n$ et $\sigma$ fixés, on modifie $\beta$ ?

C'est à vous !


# Perspectives

## Ré-écrire le modèle

La loi Normale dans l'équation de vraisemblance \eqref{eqn:likCond} est utilisée sous sa forme *univariée*, c'est-à-dire qu'on s'en sert pour "tirer" aléatoirement un seul nombre correspondant à une réalisation de la variable aléatoire d'intérêt.

Or la même loi existe aussi sous forme *multivariée*, ce qui permet, à chaque fois qu'on "tire" dedans, d'obtenir *plusieurs* nombres, ceux-ci étant arrangés dans un vecteur.
Sous cette forme, la vraisemblance s'écrit maintenant:

\begin{align}
\boldsymbol{y} \, | \, \boldsymbol{x}, \mu, \beta, \sigma \; \sim \mathcal{N}_n(\boldsymbol{1} \mu + \boldsymbol{x} \beta, \sigma^2 I_n)
\label{eqn:likCondMulti}
\end{align}

où $\boldsymbol{y}$ est le vecteur de dimension $n$ contenant les phénotypes, $\boldsymbol{x}$ est le vecteur de dimension $n$ contenant les génotypes, $\boldsymbol{1}$ est le vecteur de dimension $n$ ne contenant que des $1$, et $I_n$ est la matrice identité de dimension $n \times n$.

Si vous n'êtes pas très familier de la version multivariée, voici un exemple avec un vecteur aléatoire de longueur 2, $\boldsymbol{\theta}$, distribué selon une loi Normale bivariée $\mathcal{N}_2(\boldsymbol{\mu}, \Sigma)$.
Ce vecteur aléatoire a deux éléments, $\theta_1$ et $\theta_2$.
Le vecteur $\boldsymbol{\mu}$ a donc également deux éléments, le premier, $\mu_1$, étant l'espérance de $\theta_1$, et le deuxième, $\mu_2$, étant l'espérance de $\theta_2$.
La matrice $\Sigma$ contient les variances $\sigma_1^2$ de $\theta_1$ et $\sigma_2^2$ de $\theta_2$ sur la diagonale.
Mais hors de la diagonale elle contient la covariance $\sigma_{12}^2$ entre $\theta_1$ et $\theta_2$.
Dans ce cas simple, on retrouve donc la corrélation, $\rho$, entre $\theta_1$ et $\theta_2$ comme étant $\sigma_{12}^2 / \sqrt{\sigma_1^2 \sigma_2^2}$.
On peut donc écrire:
\begin{align}
\begin{bmatrix}
\theta_1 \\
\theta_2
\end{bmatrix}
\sim
\mathcal{N}_2 \left(
\begin{bmatrix}
\mu_1 \\
\mu_2
\end{bmatrix},
\begin{bmatrix}
\sigma_1^2 & \sigma_{12}^2 \\
\sigma_{12}^2 & \sigma_2^2
\end{bmatrix}
\right)
\Leftrightarrow
\boldsymbol{\theta} \sim \mathcal{N}_2(\boldsymbol{\mu}, \Sigma)
\end{align}

Passons au simulations, en commençant par fixer quelques valeurs:
```{r norm_bivar_param}
mu.1 <- 5
mu.2 <- 23
mu <- c(mu.1, mu.2)
var.1 <- 1
var.2 <- 2
rho <- 0.8
covar.12 <- rho * sqrt(var.1 * var.2)
Sigma <- matrix(c(var.1, covar.12, covar.12, var.2), nrow=2, ncol=2)
```

Voici une fonction permettant de simuler des vecteurs aléatoires, issue du paquet [MASS](https://cran.r-project.org/web/packages/MASS/):
```{r norm_bivar_simul}
theta <- mvrnorm(n=100, mu=mu, Sigma=Sigma)
```

Regardons à quoi les échantillons ressemblent:
```{r norm_bivar_plots}
bivn.kde <- kde2d(x=theta[,1], y=theta[,2], n=50)
image(bivn.kde, xlab=expression(theta[1]), ylab=expression(theta[2]), las=1,
      main=bquote(bold(paste("Echantillons d'une Normal bivariée avec ",
                             rho, " = ", .(rho)))))
contour(bivn.kde, add=T)
```

A vue d'oeil, on retrouve bien les valeurs utilisées pour les espérances, ainsi qu'une forte corrélation positive puisqu'on a utilisé $\rho$ = `r rho` dans les simulations.


## Améliorer le modèle

Naturellement, l'activité de modélisation statistique ne se limite pas à simuler des données sur ordinateur.
Bien au contraire, elle est au coeur de l'activité de recherche en ce qu'elle vise à identifier les caractéristiques saillantes d'un phénomène naturel afin d'en réaliser l'inférence.

Concernant le thème de l'atelier, la prédiction génomique, quelles sont les limites du modèle exploré ci-dessus ?
Que proposez-vous pour y remédier ?


# Annexe

```{r info}
t1 <- proc.time(); t1 - t0
print(sessionInfo(), locale=FALSE)
```
