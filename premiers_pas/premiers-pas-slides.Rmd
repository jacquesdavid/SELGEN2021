---
title: "Atelier prédiction génomique : <br> premiers pas"
author: "Vincent Segura (INRAE) <br><br> d'après ['premiers-pas.Rmd'](https://github.com/timflutre/atelier-prediction-genomique/raw/master/premiers-pas.Rmd) de Timothée Flutre (INRAE) <br>"
date: "Lundi 17 février 2020"
output:
  ioslides_presentation:
    widescreen: yes
    incremental: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

# Introduction

## R, rmarkdown, RStudio

- Cette présentation a été générée à partir d'un fichier texte au format Rmd utilisé par le logiciel libre [R](http://www.r-project.org/)

- La fonction `render` du package [rmarkdown](https://cran.r-project.org/web/packages/rmarkdown/index.html) permet de générer le fichier html à partir du fichier Rmd

```{r rmarkdown, eval=FALSE}
library(rmarkdown)
render("premiers-pas-slides.Rmd")
```

- Il est généralement plus simple pour faire ça d'utiliser le logiciel [RStudio](http://www.rstudio.com/)

- Le format Rmd permet également d’utiliser le language LaTeX pour écrire des équations

## Packages

- Cette présentation nécessite par ailleurs le chargement des packages [MASS](https://cran.r-project.org/web/packages/MASS/index.html) et [stats4](https://www.rdocumentation.org/packages/stats4/versions/3.6.2) qui sont généralement inclus par defaut dans R

```{r MASS stats4}
library(MASS)
library(stats4)
```

## Notation et vocabulaire {.build}

<div>
- L'inférence avec un modèle statistique consiste généralement à **estimer les paramètres**, puis à s'en servir pour **prédire de nouvelles données**
</div>

<div>
- Lorsqu’on propose un modèle, on commence par expliquer les **notations**
- **Conventions** :
    * lettres grecques pour les paramètres (non-observés), par exemple $\theta$
    * lettres romaines pour les données observées, $y$
    * lettres romaines surmontées d'un tilde pour les données prédites, $\tilde{y}$
    * les ensembles de données ou de paramètres sont généralement notés en majuscule, $\mathcal{D} = \{ y_1, y_2, y_3 \}$ ou $\Theta = \{ \theta_1, \theta_2 \}$
    * s'il y a plusieurs paramètres ou données, ils se retrouvent mathématiquement dans des vecteurs, en gras, $\boldsymbol{\theta}$ et $\boldsymbol{y}$
    * les vecteurs sont en colonne
</div>

## La notion de vraisemblance {.build}

<div>
- Une fois les notations établies, on écrit la **vraisemblance** (*likelihood*), souvent présentée comme étant la "probabilité des données sachant les paramètres"

- Si les données sont des **variables continues**, c'est la densité de probabilité des données sachant les paramètres, notée $p(y | \theta)$, si les données sont des **variables discrètes**, c'est la fonction de masse, notée $P(y | \theta)$

- La vraisemblance est une fonction des **paramètres**, d'où le fait qu'on la note $\mathcal{L}(\theta)$ ou $\mathcal{L}(\theta | y)$
</div>

<div>
- la méthode du **maximum de vraisemblance** cherche à identifier la valeur du paramètre, notée $\hat{\theta}$ par convention, qui maximise la vraisemblance <br>
$$
\begin{align}
\hat{\theta} = \text{argmax}_\theta \, \mathcal{L} \; \; \Leftrightarrow \; \; \frac{\partial \mathcal{L}}{\partial \theta}(\hat{\theta}) = 0
\end{align}
$$
</div>

## L'incertitude

- Certains décrivent les statistiques comme étant la "science de l'incertitude" ([Lindley, 2000](http://dx.doi.org/10.1111/1467-9884.00238))

- Pour l'instant, nous n'avons parlé que de la façon d'obtenir *une* valeur par paramètre, celle qui maximise la vraisemblance.

- Il est donc primordial ensuite de **quantifier l'incertitude** que nous avons quant à cette valeur

- C'est sur ce point que différentes approches sont possibles (**fréquentiste**, **bayésienne**).

## Comprendre la vraisemblance {.build}

<div>
- Supposons que l'on étudie une quantité physique dont la valeur résulte de la somme d'une très grande quantité de facteurs indépendants, chacun ayant un faible impact sur la valeur finale

- On prend trois mesures de cette quantité d'intérêt
</div>

<div>
- Comme il y a de la variation, on choisit d'introduire une **variable aléatoire** $Y$ correspondant à la quantité d'intérêt, et on dénote par $y_1$, $y_2$ et $y_3$ les trois observations, vues comme des réalisations de cette variable aléatoire : 
```{r echo=FALSE}
set.seed(1)
y <- rnorm(n=3, mean=5, sd=1)
```
<div class="centered">
$y_1$ = `r y[1]`, $y_2$ = `r y[2]`, $y_3$ = `r y[3]`
</div>
</div>

<div>
- Etant donné les caractéristiques du phénomène, il est raisonnable de supposer que la variable $Y$ suit une **loi Normale** (c.f. le théorème central limite)
</div>

----

- Cette distribution de probabilité est caractérisée par deux paramètres, sa **moyenne** que l'on note généralement $\mu$, et sa **variance** que l'on note généralement $\sigma^2$ ($\sigma$ étant l'écart-type)

- En terme de notation, on écrit $Y \sim \mathcal{N}(\mu, \sigma^2)$, et la densité de probabilité de la réalisation $y$ de $Y$ s'écrit :
$$
\begin{align}
Y \sim \mathcal{N}(\mu, \sigma^2) \; \; \Leftrightarrow \; \; p(Y=y \, | \, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y - \mu)^2}{2 \sigma^2} \right)
\end{align}
$$

- L'intérêt de ce modèle paramétrique est de pouvoir "résumer" les données, par exemple un million de mesures, par seulement deux valeurs, les paramètres

----

- Mais bien entendu, nous ne connaissons pas les valeurs de paramètres !

- La moyenne $\mu$ peut prendre toutes les valeurs entre $-\infty$ et $+\infty$, et la variance $\sigma^2$ n'a pour seule restriction que d'être positive

- La loi Normale peut être assez différente selon les valeurs de ces paramètres

<div style="position: relative; top: -30px">
```{r effect_mu effect_sigma, echo=FALSE, dev.args = list(bg = 'transparent'), fig.height=4.5, fig.width=9.5, fig.align='center'}
par(mfrow = c(1, 2))
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
x <- seq(from=-7, to=13, length.out=500)
plot(x=x, y=dnorm(x=x, mean=3, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(mu==3))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=3, sd=3), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(sigma==1), expression(sigma==3)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
```
</div>

----

<div style="position: relative; top: -10px">
- Revenons à nos trois mesures : `r y`
</div>

<div style="position: relative; top: -20px">
- Parmi toutes les valeurs possibles des paramètres, quelles sont celles pour lesquelles la loi Normale est une bonne description du mécanisme qui a généré ces données ?
</div>

<div style="position: relative; top: -30px">
- Pour simplifier, supposons que l'on connaisse déjà la variance : $\sigma^2 = 1$,<br>il ne nous reste plus qu'à trouver la moyenne : $\mu$.
</div>

<div style="position: relative; top: -40px">
- Pour la première observation, $y_1$ = `r y[1]` :
</div>

<div style="position: relative; top: -60px">
```{r ex_lik_y1, echo=FALSE, dev.args = list(bg = 'transparent'), fig.height=3.4, fig.width=6.1, fig.align='center'}
par(mar = c(4, 4, 2, 1.5))
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~vraisemblances~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=2, sd=1))
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=5, sd=1))
segments(x0=y[1], y0=dnorm(x=y[1], mean=2, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=2, sd=1), col="blue")
segments(x0=y[1], y0=dnorm(x=y[1], mean=5, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=5, sd=1), col="red")
text(x=1.1*y[1], y=0.18, labels=expression(y[1]))
text(x=-1.6, y=dnorm(x=y[1], mean=2, sd=1), col="blue", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==2,")")))
text(x=-1.6, y=dnorm(x=y[1], mean=5, sd=1), col="red", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==5,")")))
```
</div>

----

- D'après le graphique précédent : $p(y_1 \, | \, \mu=5, \sigma=1) \, > \, p(y_1 \, | \, \mu=2, \sigma=1)$

- Cela se vérifie si l'on fait le calcul avec la formule :
    * $p(y_1 \, | \, \mu=5, \sigma=1)$ = `r dnorm(x=y[1], mean=5, sd=1)`
    * $p(y_1 \, | \, \mu=2, \sigma=1)$ = `r dnorm(x=y[1], mean=2, sd=1)`

- Comme les deux densités ont la même valeur pour $\sigma$, la différence vient bien du terme $(y - \mu)^2$ dans l'exponentielle, terme qui représente l'écart à la moyenne

- Au final, nous pouvons conclure pour la première observation, que la vraisemblance $\mathcal{L}(\mu=5,\sigma=1)$ est plus grande que $\mathcal{L}(\mu=2,\sigma=1)$

----

- Comme on dispose de plusieurs observations, $\{y_1,y_2,y_3\}$, et qu'on suppose qu'elles sont toutes des réalisations de la même variable aléatoire, $Y$, il est pertinent de calculer la vraisemblance de toutes ces observations conjointement plutôt que séparément :
$$
\begin{align}
\mathcal{L}(\mu, \sigma) = p(y_1,y_2,y_3 \, | \, \mu, \sigma)
\end{align}
$$

- Si l'on fait aussi l'hypothèse que ces observations sont indépendantes, cela se simplifie en :
$$
\begin{align}
\mathcal{L}(\mu, \sigma) &= p(y_1 \, | \, \mu, \sigma) \times p(y_2 \, | \, \mu, \sigma) \times p(y_3 \, | \, \mu, \sigma) \\
&= \prod_{i=1}^3 p(y_i \, | \, \mu, \sigma)
\end{align}
$$

----

- Il n'est pas très pratique de maximiser la vraisemblance directement, on préfère passer au log (qui est monotone, donc le maximum de l'un est aussi le maximum de l'autre) :
$$
\begin{align}
l(\mu,\sigma) &= \log \mathcal{L}(\mu, \sigma) \\
&= \sum_{i=1}^3 \log p(y_i \, | \, \mu, \sigma) \\
&= \sum_{i=1}^3 \log \left[ \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu)^2}{2 \sigma^2} \right) \right] \\
&= - 3 \log \sigma \; - \frac{3}{2} \log (2\pi) \; - \frac{1}{2 \sigma^2} \sum_{i=1}^3 (y_i - \mu)^2
\end{align}
$$

----

- En pratique, (i) on écrit une fonction qui calcule la log-vraisemblance, et (ii) on cherche le maximum de cette fonction

```{r compute_lik}
compute.log.likelihood <- function(parameters, data){
  mu <- parameters[1]
  sigma <- parameters[2]
  y <- data
  n <- length(y)
  log.lik <- - n * log(sigma) - (n/2) * log(2 * pi) - sum(((y - mu)^2) / (2 * sigma^2))
  return(log.lik)
}

compute.log.likelihood(c(5,1), y)
compute.log.likelihood(c(2,1), y)
```

----

- Dans le cas de la loi Normale, il existe déjà dans R des fonctions implémentant la densité de probabilité, ce qui nous permet de vérifier que nous n'avons pas fait d'erreur

```{r check_compute_lik}
sum(dnorm(x=y, mean=5, sd=1, log=TRUE))
sum(dnorm(x=y, mean=2, sd=1, log=TRUE))
```

# Ecrire le modèle

## Notations

<div style="position: relative; top: -20px">
- $n$ : nombre d'individus (diploïdes, supposés non-apparentés)
</div>

<div style="position: relative; top: -25px">
- $i$ : indice indiquant le $i$-ème individu, $i \in \{1,\ldots,n\}$
</div>

<div style="position: relative; top: -30px">
- $y_i$ : phénotype de l'individu $i$ pour la caractère d'intérêt
</div>

<div style="position: relative; top: -35px">
- $\mu$ : moyenne globale du phénotype des $n$ individus
</div>

<div style="position: relative; top: -40px">
- $f$ : fréquence de l'allèle minoritaire au marqueur SNP d'intérêt
</div>

<div style="position: relative; top: -45px">
- $x_i$ : génotype de l'individu $i$ à ce SNP, codé comme le nombre de copie(s) de l'allèle minoritaire, $\forall i \; x_i \in \{0,1,2\}$
</div>

<div style="position: relative; top: -50px">
- $\beta$ : effet additif de chaque copie de l'allèle minoritaire en unité du phénotype
</div>

<div style="position: relative; top: -55px">
- $\epsilon_i$ : erreur pour l'individu $i$
</div>

<div style="position: relative; top: -60px">
- $\sigma^2$ : variance des erreurs
</div>

## Notations (suite)

- Données : $\mathcal{D} = \{ (y_1 \, | \, x_1), \ldots, (y_n \, | \, x_n) \}$

- Paramètres : $\Theta = \{ \mu, \beta, \sigma \}$

## Vraisemblance

- On suppose que le génotype au SNP d'intérêt a un effet additif sur la moyenne du phénotype, ce qui s'écrit généralement :
$$
\begin{align}
\forall i \; \; y_i = \mu + \beta x_i + \epsilon_i \text{ avec } \epsilon_i \overset{\text{i.i.d}}{\sim} \mathcal{N}(0, \sigma^2)
\end{align}
$$

- Une autre façon équivalente de l'écrire :
$$
\begin{align}
\forall i \; \; y_i \, | \, x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\end{align}
$$

# Simuler des données

----

- Initialisation :<br><br>
On utilise un générateur de nombres pseudo-aléatoires qui peut être initialisé avec une graine (seed), ce qui est très utile pour la reproductibilité des analyses

```{r set_seed}
set.seed(1866) # année de parution de l'article de Mendel fondant la génétique
```

- Nombre d'individus :

```{r set_sample_size}
n <- 200
```

- Moyenne générale :

```{r set_global_mean}
mu <- 50
```

----

- Génotypes (on suppose que la population est à l'équilibre d'Hardy-Weinberg) :

```{r simul_geno}
##' Genotype frequencies
##'
##' Calculate the genotype frequencies at a locus assuming the Hardy-Weinberg equilibrium
##' (https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle).
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
##' @author Timothee Flutre
calcGenoFreq <- function(maf){
  stopifnot(is.numeric(maf), length(maf) == 1, maf >= 0, maf <= 0.5)
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}
f <- 0.3
genotypes <- sample(x=c(0,1,2), size=n, replace=TRUE, prob=calcGenoFreq(f))
```

----

```{r look_geno}
head(genotypes)
table(genotypes)
sum(genotypes) / (2 * n) # estimate of the MAF
var(genotypes) # important for the estimate of beta
```

----

- Effet du génotype sur le phénotype, $\beta$ :

```{r simul_geno_effect}
(beta <- rnorm(n=1, mean=2, sd=1))
```

- Erreurs, $\boldsymbol{\epsilon}$ (par simplicité, on fixe $\sigma$ à 1) :
```{r simul_errors}
sigma <- 1
errors <- rnorm(n=n, mean=0, sd=sigma)
```

----

- Nous avons maintenant tout ce qu'il faut pour simuler les phénotypes, $\boldsymbol{y}$, via l'équation précédente : $y_i = \mu + \beta x_i + \epsilon_i$

```{r simul_pheno}
phenotypes <- mu + beta * genotypes + errors
```

- Il est habituel dans R d'organiser les données dans un tableau

```{r org_data}
dat <- data.frame(x=genotypes, y=phenotypes)
head(dat)
```

# Réaliser l'inférence

## Visualisation graphique

- Distribution du phénotype

```{r look_pheno, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4, fig.align='center'}
par(mar = c(4, 4, 1, 1))
hist(phenotypes, las = 1, main = "",
     xlab = expression(paste("Phénotypes, ", bold(y))),
     ylab = "Nombre d'individus")
```

----

- Relation génotypes - phénotypes

```{r look_geno_pheno, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4.5, fig.align='center'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab = expression(paste("Génotypes, ", bold(x))),
        ylab = expression(paste("Phénotypes, ", bold(y))),
        varwidth = TRUE, notch = TRUE, las = 1, xaxt = "n", at = 0:2)
axis(side = 1, at = 0:2, labels = c("0 (AA)", "1 (Aa)", "2 (aa)"))
```

## Implémentation (facile)

- Sous R, la fonction `lm` implémente l'estimation par maximum de vraisemblance

```{r fit_lm}
fit <- lm(y ~ x, data=dat)
```

- Vérification des hypothèses du modèle (homoscédasticité, normalité, indépendance)

```{r diagnostics, fig.show='hide'}
par(mfrow=c(2, 2), mar = c(4, 4, 2, 1))
plot(fit)
```

----

```{r diagnostics plot, echo = FALSE, dev.args = list(bg = 'transparent'), fig.height=6, fig.width=6, fig.align='center'}
par(mfrow=c(2, 2), mar = c(4, 4, 2, 1))
plot(fit)
```

----

```{r get_results_summary}
summary(fit)
```

----

- Représentation graphique du modèle

```{r look_geno_pheno_reg code, fig.show='hide'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright",
       legend=expression(hat(y)[i]==hat(mu)~+~hat(beta)~x[i]),
       col="red", lty=1, bty="n")
```

----

```{r look_geno_pheno_reg, echo = FALSE, dev.args = list(bg = 'transparent'), fig.height=5.5, fig.width=6.5, fig.align='center'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright",
       legend=expression(hat(y)[i]==hat(mu)~+~hat(beta)~x[i]),
       col="red", lty=1, bty="n")
```

## Implémentation (plus difficile)

- Il faut d'abord écrire une fonction calculant l'opposé de la log-vraisemblance

```{r neg_log_lik}
negLogLik <- function(mu, beta, sigma){
  - sum(dnorm(x=dat$y, mean=mu + beta * dat$x, sd=sigma, log=TRUE))
}
```

- Puis demander à la fonction `mle` de la maximiser (en spécifiant que le paramètre $\sigma$ ne peut pas être négatif ou nul)

```{r fit_mle}
fit2 <- mle(negLogLik, start=list(mu=mean(dat$y), beta=0, sigma=1),
            method="L-BFGS-B", nobs=nrow(dat),
            lower=c(-Inf,-Inf,10^(-6)),
            upper=c(+Inf,+Inf,+Inf))
```

----

```{r summary fit_mle}
summary(fit2)
```

# Evaluer les résultats

## Sélection de modèles

- Evaluation de l'ajustement du modèle aux données

- Dans notre cas de régression linéaire simple, on peut utiliser le coefficient de détermination $R^2$

```{r adjust}
summary(fit)$r.squared
```

----

- On peut facilement vérifier que cette valeur renvoyée par la fonction `lm` correspond à la formule :

$$
\begin{align}
R^2 = \frac{\hat{\beta}^2 \; Var(\boldsymbol{x})}{\hat{\beta}^2 \; Var(\boldsymbol{x}) + \hat{\sigma}^2}
\end{align}`
$$
```{r calc_adjust}
(coefficients(fit)[2]^2 * var(dat$x)) /
  (coefficients(fit)[2]^2 * var(dat$x) + summary(fit)$sigma^2)
```

## Estimation des paramètres

- Prenons l'exemple de $\beta$, comme nous avons simulé les données, nous connaissons sa vraie valeur

```{r true_beta}
beta
```

- Après avoir ajusté le modèle avec la fonction `lm`, nous pouvons récupérer l'estimation de ce paramètre ($\hat{\beta}$)

```{r estim_beta}
(beta.hat <- coefficients(fit)[2])
```

----

- Pour comparer les deux, on définit une fonction de perte (*loss function*) reliant le paramètre ($\beta$) à son estimation ($\hat{\beta}$)

- On utilise une fonction quadratique, dont on prend l'espérance, ce qui donne l'erreur quadratique moyenne (*mean squared error*)

$$
\begin{align}
MSE = E \left( (\hat{\beta} - \beta)^2 \right)
\end{align}
$$

- On calcule sa racine carrée pour que le résultat soit dans la même unité que le paramètre

```{r rmse_beta}
(rmse.beta <- sqrt((beta.hat - beta)^2))
```

## Prédiction de données

- On peut aussi calculer l'erreur quadratique moyenne avec les phénotypes déjà observés (on parle de *in-sample predictions*)

```{r calc_rmse_y}
y <- phenotypes
y.hat <- (coefficients(fit)[1] + coefficients(fit)[2] * genotypes)
errors <- y - y.hat
(rmse.y <- sqrt(mean(errors^2)))
```

- On peut aussi utiliser la fonction `predict`

```{r rmse_y}
errors <- phenotypes - predict(fit)
(rmse.y <- sqrt(mean(errors^2)))
```

----

- Le vecteur *errors* correspond aux résidus du modèle

```{r resid}
head(errors)
head(resid(fit))
(rmse.y <- sqrt(mean(resid(fit)^2)))
```

----

- De façon plus intéressante, on souhaiterai évaluer les prédictions phénotypiques sur $n_{\text{new}}$ nouveaux individus

- Pour cela, on commence par simuler de nouvelles données, $\mathcal{D}_{\text{new}} = \{(y_{i,\text{new}} \, | \, x_{i,\text{new}})\}$, toujours avec les *mêmes* "vraies" valeurs des paramètres, $\Theta = \{\mu, \beta, \sigma\}$

```{r simul_new_data}
set.seed(1944) # année de découverte de l'ADN comme support des gènes
n.new <- 100
x.new <- sample(x=c(0,1,2), size=n.new, replace=TRUE, prob=calcGenoFreq(f))
y.new <- mu + beta * x.new + rnorm(n=n.new, mean=0, sd=sigma)
```

----

- Puis on utilise les estimations des paramètres obtenues précédemment pour prédire les nouveaux phénotypes à partir des nouveaux génotypes, $\tilde{\mathcal{D}}_{\text{new}} = \{(\tilde{y}_{i,\text{new}} = \hat{\mu} + \hat{\beta} \, x_{i,\text{new}})\}$ (*out-of-sample predictions*)

```{r predict_ytilde}
y.new.tilde <- (coefficients(fit)[1] + coefficients(fit)[2] * x.new)
```

- Enfin, on calcule l'erreur quadratique moyenne de prédiction

```{r rmspe}
errors.tilde <- y.new - y.new.tilde
(rmspe <- sqrt(mean(errors.tilde^2)))
```

----

Graphiquement :

```{r ynew_vs_ytilde, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4, fig.align='center'}
par(mar = c(4, 4.5, 1, 1))
plot(x=y.new, y=y.new.tilde, las=1,
     xlab=expression(paste("Nouveaux vrais phénotypes, ", bold(y)[new])),
     ylab=expression(paste("Nouveaux phénotypes prédits, ", bold(tilde(y))[new])))
abline(a=0, b=1, lty=2)
legend("topleft", legend="ligne identité", lty=2, bty="n")
```

# Perspectives

## Explorer les simulations possibles

- La simulation est un outil particulièrement utile pour explorer comment un modèle répond à des changements dans les données et les paramètres

- On pourrait par exemple avoir envie de savoir ce qui se passe si la taille de l'échantillon ($n$) varie

- Idem, que se passe-t-il si, à $n$ et $\sigma$ fixés, on modifie $\beta$ ?

<!-- <div class="centered" style="font-size:40px; position: relative; top: 20px"> -->
<!-- **C'est à vous de jouer !** -->
<!-- </div> -->

## Ré-écrire le modèle

- Modèle sous forme *multivariée*
$$
\begin{align}
\boldsymbol{y} \, | \, \boldsymbol{x}, \mu, \beta, \sigma \; \sim \mathcal{N}_n(\boldsymbol{1} \mu + \boldsymbol{x} \beta, \sigma^2 I_n)
\end{align}
$$
- avec :
    * $\boldsymbol{y}$ : vecteur de dimension $n$ contenant les phénotypes,
    * $\boldsymbol{x}$ : vecteur de dimension $n$ contenant les génotypes,
    * $\boldsymbol{1}$ : vecteur de dimension $n$ ne contenant que des $1$,
    * $I_n$ : matrice identité de dimension $n \times n$

----

- Exemple d'un vecteur aléatoire de longueur 2, $\boldsymbol{\theta}$, distribué selon une loi Normale bivariée $\mathcal{N}_2(\boldsymbol{\mu}, \Sigma)$ :
$$
\boldsymbol{\theta} \sim \mathcal{N}_2(\boldsymbol{\mu}, \Sigma)
\Leftrightarrow
\begin{align}
\begin{bmatrix}
\theta_1 \\
\theta_2
\end{bmatrix}
\sim
\mathcal{N}_2 \left(
\begin{bmatrix}
\mu_1 \\
\mu_2
\end{bmatrix},
\begin{bmatrix}
\sigma_1^2 & \sigma_{12}^2 \\
\sigma_{12}^2 & \sigma_2^2
\end{bmatrix}
\right)
\end{align}
$$

----

- On fixe quelques valeurs

```{r norm_bivar_param}
mu.1 <- 5
mu.2 <- 23
mu <- c(mu.1, mu.2)
var.1 <- 1
var.2 <- 2
rho <- 0.8
covar.12 <- rho * sqrt(var.1 * var.2)
Sigma <- matrix(c(var.1, covar.12, covar.12, var.2), nrow=2, ncol=2)
```

----

- On simule des vecteurs aléatoires avec la fonction `mvrnorm` du package [MASS](https://cran.r-project.org/web/packages/MASS/index.html)

```{r norm_bivar_simul}
theta <- mvrnorm(n=100, mu=mu, Sigma=Sigma)
dim(theta)
head(theta)
```

----

- Représentation graphique

```{r norm_bivar_plots, dev.args = list(bg = 'transparent'), fig.height=4, fig.width=5.5, fig.align='center'}
bivn.kde <- kde2d(x=theta[,1], y=theta[,2], n=50)
image(bivn.kde, xlab=expression(theta[1]), ylab=expression(theta[2]), las=1,
      main=bquote(bold(paste("Echantillons d'une Normal bivariée avec ",
                             rho, " = ", .(rho)))))
contour(bivn.kde, add=T)
```

# Annexe

----

<div style="position: relative; top: -50px">
```{r info}
print(sessionInfo(), locale=FALSE)
```
</div>
